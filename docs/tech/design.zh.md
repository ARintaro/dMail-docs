# 软件设计文档

## 项目背景

当下，微信、Telegram等即时通讯服务已经成为了个人或者组织在互联网上进行信息交换的重要工具。然而，对终端用户而言，使用公有IM进行信息交流和数据传输时面临着极高的信息外泄风险，公有IM中的聊天信息、文件、数据均保存在运营商手中，用户无法进行有效管控。其中绝大多数的公有IM因其盈利性质，客户端与服务端均为闭源，用户也无从得知运营商关于信息安全的承诺是否有效。所以，使用开源项目部署私有化的IM服务，才能最大程度的保护用户的信息安全。

但是目前开源社区中即时通讯相关的生态并不完善，项目数量较少。本项目在这种背景下，试图给出一个在部署成本、可靠性、性能这三者中取得平衡的一个私有IM部署方案。

## 项目目标

为用户提供一个易于部署、高性能、安全的私有IM部署方案。

## 技术选型概览

对于多数中小IM部署者而言，无法承担分布式部署的成本，因此单机性能就十分重要，需要能在一个连接服务器处理大量的并发请求。另一方面，系统也需要有足够的横向扩展能力，满足服务更多用户的需求。

因此，总体的技术栈选型思路是牺牲开发速度与功能上的丰富性，换取性能。

连接服务端的开发语言选用`Rust`。最优先的考虑肯定是性能与内存安全，但除此之外，`Rust`完善的错误处理机制也能提高代码质量；作为编译型语言，也降低了用户部署成本，无需额外安装运行时或者复杂的依赖。强大的泛型能力做到了零运行时成本抽象，主要代码的抽象依赖于泛型与条件编译，而非虚表。

Web端选用`React`框架，使用`Typescript`作为开发语言，`Mobx`为状态管理库。相对于传统Web应用的B/S架构，即时通讯系统作为一个单页应用程序其实更像一个C/S程序，`React`原生的写法难以做到数据与渲染的分离，累计足够多的功能代码后会导致项目代码可维护性大大下降。另外，由于游览器的限制，只能使用`WebSocket`保持长连接，所以在`WebSocket`上二次封装了协议。

后端数据库层实现了完整的抽象，使用条件编译切换不同的数据库。但由于课程时间限制，仅兼容了`Redis`协议数据库。目前实际用于部署的`Pika`，开源的可持久化兼容`Redis`协议的分布式数据库。SQL虽然提供了丰富的功能，但是QPS远远无法满足即时通讯的需求，分布式扩展也需要更改后端逻辑，手动去做分库分表等逻辑，无法像KV数据库一样方便地部署分布式。


## 技术选型分析

### 连接服务端架构分析

选用`Actix Web`作为Web服务的框架，同时所有需要等待IO的操作全部为异步实现，无阻塞。

连接服务端同时存在两个`Tokio`运行时，分别运行在两个线程池上。

第一个线程池负责处理连接和简单请求，`Actix`框架提供的Actor也运行在这个线程池上。用户连接所使用的`Actor`异步模型可以保证用户线程即便发生崩溃也不会影响后端的正常服务，同时提供了方便的分布式拓展方案。

第二个线程池负责高负载任务，例如读扩散的消息拉取和。这种负载任务的特点在于除了大量计算和字符串拼接之外，还需要大量的时间等待IO。我们不希望这种任务会影响用户连接，需要放在单独的线程池上运行，但是朴素的线程池只能同步运行。因此最终选择了再启动一个`Tokio`运行时去运行这些任务。当然在核心数不够的机器上这很有可能是一个负优化。

负责维护连接的哈希表使用了分布式锁，尽可能地减少了锁冲突。其实除这个哈希表外，后端服务几乎是无状态的，信息全部存储在数据库上。

这些设计给连接服务端提供了足够强的横向扩展能力，单机核心数的提高可以保证性能的提升。单次请求处理速度的主要瓶颈其实是在数据库端，基于这个事实，分布式部署的数据库结合单机的连接服务端即可处理大量的用户请求。

### 通信协议设计

由于私有部署的IM服务可能由于各种原因没有域名从而无法进行HTTPS证书签名，我们自己实现了一套类似HTTPS的加密协议，尽管没有第三方签名，它无法避免中间人攻击。目前在Secoder与腾讯云上部署的测试服务同时启用了这套加密协议与HTTPS，虽然在启用HTTPS后其实就没有必要启用这套协议了。（另外，这套协议增大了攻击者的调试成本，想要直接与后端通信需要自行实现这套加密协议）

具体地，建立WebSocket连接时客户端向服务端发送RSA公钥，服务端使用AES-GCM-128生成对称加密密钥，并使用用户的RSA公钥加密对称秘钥发送给用户，客户端使用RSA私钥解密后，使用对称加密密钥交换信息。此后所有信息均会通过加密后的WebSocket信道发送，不会再进行HTTP连接，这同时也提高了服务端性能。

### 数据库选型

传统的Web应用一般会使用SQL数据库存储信息，但对于一个IM服务来讲，SQL的QPS太低，分布式拓展很麻烦，需要手动分库分表，逻辑会侵入业务。同时，无论是写扩散还是读扩散，都需要维护大量的Timeline，在SQL中只能在一个表格内存储多个Timeline，这样对于单个Timeline的写入/读取操作都会与整个表格的对数规模相关，而非仅与单个Timeline的规模有关。而KV数据库可以自然地把这些Timeline分开，单次操作的复杂度更优。也易于支持分布式，可以把不同的Timeline分布到不同的机器上。缺点在于社区提供的开源分布式KV数据库普遍不支持事务，对数据的严肃性不高。另外一个方案是SQL加上`Redis`缓存优化，这种方案的麻烦之处在于对缓存的更新、查询、写入会入侵业务代码，写起来更麻烦。

实践中，众多KV数据库都兼容`Redis`协议，我们也因此选择了`Redis`协议。目前线上部署的测试服务使用的是360开源的`Pika`，另外一个备选方案是腾讯开源出来的`Tendis`。

### 缓存条件下的数据扩散

Web端使用了`LocalForge`对几乎全部数据进行了缓存，避免了反复从服务器拉取，同时只有用户真正看到某些信息时才会从服务器拉取。

实现缓存后，客户端的逻辑变成「如果本地有这条数据，便不从服务端拉取」，但在很多情况下，这条数据往往不是「不可变的」。比如说一个用户的头像和昵称，这种情况下客户端如果不再拉取就无法与远端的最新数据保持同步。对于这种可变数据，我们与微信一致，用户点开其他用户的具体信息时再重新从远端拉取数据。

另一个麻烦的功能点在于「撤回」，本地任何一条已有的聊天信息，都有可能在远端实际上也被撤回。如果撤回时客户端离线，客户端就无法判断本地的缓存是否有效，导致缓存失效每次都需要从服务端拉取消息装填。我们解决这个问题的办法是对每个用户单独开设一个Timeline（也可以理解为写扩散），用于存储「撤回」这种必须实时接收的对客户端缓存数据的修改。

另外，由于实现了增量拉取，每个聊天在登录时不会全量拉取信息，而是仅会拉取最后的固定条数。这对于「提及」功能点来说也是一个问题，因为实际@用户的消息很可能不会被客户端拉取到，客户端也无从得知自己被提及了。解决这个问题还是在上面提及的Timeline中发送写扩散信息。

而这个Timeline本身使用一个客户端存储的时间戳进行增量拉取，拉取后同步服务端时间戳。

### OSS直传

即时通讯服务需要能够传输二进制对象，而传输二进制文件会严重占用服务器带宽和磁盘资源，所以需要与连接服务端分离，使得传输服务不占用连接服务资源。因此我们希望用户能够直接向对象上传服务端上传文件，同时完成文件权限的控制和文件的检查。下面描述完整的文件上传过程：

1. 客户端向连接服务端发送请求，附带即将上传的文件哈希与文件大小
2. 连接服务端检查请求是否合法（哈希是否存在，大小是否合法），通过检验后生成文件GUID，并返回预签名的文件上传URL，用于用户直接向对象服务端上传文件。
3. 客户端向对象服务端直接上传文件，上传成功后发送消息给连接服务端。
4. 连接服务端接收到「上传完毕」消息后，向对象上传服务端查询GUID对应的文件哈希与大小，与最初客户端发送的文件哈希、大小比对，以确认客户端在第一步请求时是否欺骗服务端。
5. 如果检验正确向用户返回预签名的下载链接，检验失败对象服务端删除文件。

实践上选择了兼容Amazon S3协议，目前线上部署的测试服务选用的开源对象存储服务`Minio`，其实功能上相对成熟的商业闭源项目较弱。

### 音视频通话

使用了WebRTC技术实现视频通话，这也是在游览器重重限制下做视频通话几乎唯一的方案。在复杂网络环境时内网穿透建立P2P连接时常失败，因此我们也部署了COTURN服务器作为TURN服务用于穿透失败时的流量中转。

### 虚拟DOM

消息列表使用了`Virtuso`库实现了虚拟DOM，仅会将有限的节点实际加入DOM树，并在用户滚动时实时切换。避免了前端渲染大量消息时的卡顿。



